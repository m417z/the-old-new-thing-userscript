<html>
<head>
<title>When I tell Windows to compress a file, the compression is far worse than I get if I ask WinZip to compress the file; why is that?</title>
<link rel="stylesheet" href="page.css">
</head><body>
<div class="titlediv"><h2>When I tell Windows to compress a file, the compression is far worse than I get if I ask WinZip to compress the file; why is that?</h2></div>
<div class="hdrdiv"><table class="hdrtable" cellspacing="0" cellpadding="0">
<tr><td><b>Date:</b></td><td>July 18, 2016 / year-entry #149</td></tr>
<tr><td><b>Tags:</b></td><td>tipssupport</td></tr>
<tr><td><b>Orig Link:</b></td><td>https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895</td></tr>

<tr><td><b>Comments:&nbsp;&nbsp;&nbsp;&nbsp;</b></td><td>33</td></tr>
<tr><td valign="top"><b>Summary:</b></td><td valign="top">The different compressors have different goals.</td></tr>
</table></div>
<hr/>
<table class="contenttable" cellspacing="0" cellpadding="0"><tr><td><div class="contentdiv">
<!-- CONTENT START -->
<p>
A customer noted that when they took a very large (multiple gigabyte)
file and went to the file's Properties and set
"Compress contents to save disk space",
the file shrunk by 25%.
And then they needed to copy the file to a USB stick,
so they used an old copy of WinZip to compress the file,
and the result was half the size of the original.
</p>
<p>
Why is it that an old 10-year-old program can compress
files so much better than Windows 2012's built-in disk compression?
Is the NTFS compression team a bunch of lazy good-for-nothings?
</p>
<p>
Transparent file compression such as that used by NTFS
has very different requirements from archival file compression
such as that used by WinZip.
</p>
<p>
Programs like WinZip are not under time constraints;
they can take a very long time to analyze the data in order
to produce high compression ratios.
Furthermore,
the only operations typically offered by these programs are
"Compress this entire file" and "Uncompress this entire file".
If you want to read the last byte of a file, you have to
uncompress the whole thing and throw away all but the last byte.
If you want to update a byte in the middle of the file,
you have to uncompress it, update the byte, then recompress the whole thing.
</p>
<p>
Transparent file compression, on the other hand, is under real-time
pressure.
Programs expect to be able to seek to a random position in a file
and read a byte;
they also expect to be able to seek to a random position in a file and
write a byte,
leaving the other bytes of the file unchanged.
And these operations need to be <var>O</var>(1), or close to it.
</p>
<p>
In practice, what this means is that the original file is broken up
into chunks,
and each chunk is compressed independently by an algorithm that
strikes a balance between speed and compression.
Compressing each chunk independently means that you can uncompress
an arbitrary chunk of a file
without having to uncompress any chunks that it is dependent upon.
However, since the chunks are independent,
they cannot take advantage of redundancy that is present
in another chunk.
(For example, if two chunks are identical, they still need to be
compressed separately; the second chunk cannot say "I'm a copy of
that chunk over there.")
</p>
<p>
All this means that transparent file compression must sacrifice
compression for speed.
That's why its compression looks lousy when compared to an
archival compression program, which is under no speed constraints.</p>
<!-- CONTENT END -->
</div></td></tr></table>
<hr/>
<table class="commenttable" cellspacing="0" cellpadding="0"><tr><td><div class="commentdiv"><div class="commentdivhdr">
<!-- COMMENTS START -->
Comments (33)	</div>

	
			<div class="navigation pagination clear-both">
					</div>

		<ol class="comment-list">
					<li class="comment even thread-even depth-1 parent" id="comment-1256425">
				<div id="div-comment-1256425" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='http://cmdrkeene.com' rel='external nofollow' class='url'>CmdrKeene</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256425">
			July 18, 2016 at 8:00 am</a>		</div>

		<p>Another life lesson: don&#8217;t attribute to malice or incompetence what is probably just your own ignorance.</p>

		
				</div>
		<ol class="children">
		<li class="comment odd alt depth-2" id="comment-1256485">
				<div id="div-comment-1256485" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Mr Cranky</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256485">
			July 18, 2016 at 10:31 am</a>		</div>

		<p>Nice corollary to Lazarus Long&#8217;s famous axiom.</p>

		
				</div>
		</li><!-- #comment-## -->
</ol><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even thread-odd thread-alt depth-1" id="comment-1256435">
				<div id="div-comment-1256435" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Jane's Fleet Command</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256435">
			July 18, 2016 at 8:54 am</a>		</div>

		<p>When can we look forward to NTFS supporting the O(-1) space/time performance of the Pied Piper compression algorithm?</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-even depth-1" id="comment-1256445">
				<div id="div-comment-1256445" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='http://www.philipstorry.net' rel='external nofollow' class='url'>Philip Storry</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256445">
			July 18, 2016 at 9:01 am</a>		</div>

		<p>Many, many, many moons ago &#8211; in the days of Stac drive compression &#8211; you could recompress data with a higher compression level, per-file, using a supplied utility. It was something I often did with the help of a 4DOS batch file that checked the dates, and recompressed data that hadn&#8217;t been touched for ages.<br />
Of course, the downside was that if you did so, they took a lot longer to save if you did edit then them &#8211; as you rightly say.</p>
<p>I always wondered why the obvious trade-off wasn&#8217;t made &#8211; allow a high compression method, but it makes the file read-only. (I suppose my batch file could have done that anyway!)</p>
<p>Such wondering was simply an academic exercise though. A quick glance at ever-tumbling storage costs answered the question &#8211; it&#8217;s not worth it, because a bigger hard disk is always getting cheaper.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-odd thread-alt depth-1 parent" id="comment-1256455">
				<div id="div-comment-1256455" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='http://english.rejbrand.se/' rel='external nofollow' class='url'>Andreas Rejbrand</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256455">
			July 18, 2016 at 9:15 am</a>		</div>

		<p>Also, Microsoft Windows can create ZIP files without third-party software since Windows ME. Using this feature, I suppose you get results close to WinZip&#8217;s (although I haven&#8217;t used third-party compression software since ME).</p>

		
				</div>
		<ol class="children">
		<li class="comment odd alt depth-2 parent" id="comment-1256465">
				<div id="div-comment-1256465" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Yuri Khan</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256465">
			July 18, 2016 at 9:37 am</a>		</div>

		<p>Yes, and the resulting ZIP archives have invalid file name encoding until Windows 7 or some non-default updates for XP.</p>

		
				</div>
		<ol class="children">
		<li class="comment byuser comment-author-oldnewthing bypostauthor even depth-3 parent" id="comment-1256475">
				<div id="div-comment-1256475" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/Raymond+Chen+-+MSFT' rel='external nofollow' class='url'>Raymond Chen - MSFT</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256475">
			July 18, 2016 at 9:40 am</a>		</div>

		<p>I think it&#8217;s rather unfair to require Windows XP to support a feature that didn&#8217;t exist at the time it was written.</p>

		
				</div>
		<ol class="children">
		<li class="comment odd alt depth-4" id="comment-1256545">
				<div id="div-comment-1256545" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Yuri Khan</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256545">
			July 19, 2016 at 12:21 am</a>		</div>

		<p>Hm, youâ€™re right. UTF-8 was added to the ZIP spec as an official feature in version 6.3.0 on 2006-09-29. Before that, the subject of file name encoding was not even discussed in the spec, technically making ZIP an unsuitable format for cross-locale interoperation.</p>

		
				</div>
		</li><!-- #comment-## -->
</ol><!-- .children -->
</li><!-- #comment-## -->
</ol><!-- .children -->
</li><!-- #comment-## -->
</ol><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-1256495">
				<div id="div-comment-1256495" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Ben L</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256495">
			July 18, 2016 at 10:48 am</a>		</div>

		<p>I&#8217;ve always been impressed with the &#8220;Transparent file compression.&#8221; It works flawlessly.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment byuser comment-author-dwalker-wk odd alt thread-odd thread-alt depth-1" id="comment-1256505">
				<div id="div-comment-1256505" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/DWalker' rel='external nofollow' class='url'>DWalker</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256505">
			July 18, 2016 at 11:14 am</a>		</div>

		<p>Yes, these constraints to allow reading a byte from the middle of a file, and changing a byte in the middle of the file, do exist&#8230; and we have to allow for that.</p>
<p>I wonder what percentage of file reads are sequential, full-file reads?  I&#8217;ll bet it&#8217;s a high percentage.   </p>
<p>Aren&#8217;t EXE and DLL files always read sequentially, in their entirety?  And most data files including Microsoft Office files (Word, Excel, Powerpoint, maybe not Access) are too.  Not that this helps the real-time disk compression requirements..</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment byuser comment-author-cheong even thread-even depth-1" id="comment-1256535">
				<div id="div-comment-1256535" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/cheong00' rel='external nofollow' class='url'>cheong00</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256535">
			July 18, 2016 at 6:33 pm</a>		</div>

		<p>On the other hand, I wonder when the industry will settle the choice of ZIP64 and Deflate64. This makes choosing ZIP format for moving large files around on Windows and *nix systems undesirable.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment byuser comment-author-raykoopa odd alt thread-odd thread-alt depth-1 parent" id="comment-1256555">
				<div id="div-comment-1256555" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/Ray+Koopa' rel='external nofollow' class='url'>Ray Koopa</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256555">
			July 19, 2016 at 2:06 am</a>		</div>

		<p>And then I experienced that my disk usage actually went up noticably when telling it to compress my drive. I think it was like 120GB free of 300GB data, and after applying this disk compression stuff, I had only 80GB free&#8230; not sure about exact numbers anymore, I fear it since then and never tried that again. Did I just hallucinate or why is that?</p>

		
				</div>
		<ol class="children">
		<li class="comment even depth-2 parent" id="comment-1256565">
				<div id="div-comment-1256565" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">xcomcmdr</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256565">
			July 19, 2016 at 2:46 am</a>		</div>

		<p>I was constantly using it since the XP era with no problem.</p>
<p>Once I got a SSD, I experienced what you described.</p>
<p>I don&#8217;t the drive type is related, but since then I stopped using it.</p>
<p>Plus once a whole drive is compressed, you&#8217;ve got a massive case of fragmentation to resolve with Windows&#8217; defragmenter (only a problem for HDDs, obviously), which can take days on XP-era machines.</p>

		
				</div>
		<ol class="children">
		<li class="comment byuser comment-author-raykoopa odd alt depth-3" id="comment-1256615">
				<div id="div-comment-1256615" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/Ray+Koopa' rel='external nofollow' class='url'>Ray Koopa</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256615">
			July 19, 2016 at 7:55 am</a>		</div>

		<p>It was quite some time ago, in 2011 or so, I had an HDD back then. It&#8217;s crazy it happened to you too. I also think it had something to do with fragmentation&#8230;</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even depth-3 parent" id="comment-1256675">
				<div id="div-comment-1256675" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">M Hotchin</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256675">
			July 19, 2016 at 1:39 pm</a>		</div>

		<p>Perhaps fragmentation was the problem &#8211; if all the files are highly fragmented, the MFT has to grow in order to records the huge number of fragments.  Is there a tool that shows the size of the file system metadata, like the MFT and the indexes?</p>

		
				</div>
		<ol class="children">
		<li class="comment odd alt depth-4" id="comment-1256685">
				<div id="div-comment-1256685" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">M Hotchin</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256685">
			July 19, 2016 at 1:56 pm</a>		</div>

		<p>Ah, it looks like CHKDSK will at least give an overview.  Doesn&#8217;t give much detail though.<br />
    176524 KB in 57045 indexes.<br />
         0 KB in bad sectors.<br />
    503083 KB in use by the system.<br />
     65536 KB occupied by the log file.</p>

		
				</div>
		</li><!-- #comment-## -->
</ol><!-- .children -->
</li><!-- #comment-## -->
</ol><!-- .children -->
</li><!-- #comment-## -->
</ol><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1 parent" id="comment-1256566">
				<div id="div-comment-1256566" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Neil</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256566">
			July 19, 2016 at 3:25 am</a>		</div>

		<p>This of course begs the question as to how the compressed chunks are stored; for example a write operation might increase the compressed size of a chunk.</p>

		
				</div>
		<ol class="children">
		<li class="comment odd alt depth-2" id="comment-1256576">
				<div id="div-comment-1256576" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Chris Long</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256576">
			July 19, 2016 at 4:10 am</a>		</div>

		<p>Apparently, the compressed file is a special case of a sparse file, which allows some magic to happen&#8230;:</p>
<p><a href="https://blogs.msdn.microsoft.com/ntdebugging/2008/05/20/understanding-ntfs-compression/" rel="nofollow">https://blogs.msdn.microsoft.com/ntdebugging/2008/05/20/understanding-ntfs-compression/</a></p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment byuser comment-author-fredericmagnyf even depth-2" id="comment-1256595">
				<div id="div-comment-1256595" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/Medinoc' rel='external nofollow' class='url'>Medinoc</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256595">
			July 19, 2016 at 7:17 am</a>		</div>

		<p>I&#8217;ve worked on something that dealt with compressed files and fragmentation: chunks are stored back-to-back as individual &#8220;extents&#8221; of a file (which is easy to detect to consider all consecutive extents a single fragment), so my first guess would be that a chunk changing size would have to be re-written somewhere else, fragmenting the file.</p>

		
				</div>
		</li><!-- #comment-## -->
</ol><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-1256587">
				<div id="div-comment-1256587" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">MarcK4096</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256587">
			July 19, 2016 at 7:17 am</a>		</div>

		<p>I bet Windows 2012 R2 deduplication would have been more competitive.  I&#8217;ve never been a fan of NTFS compression, but I think Windows Server 2012 R2 dedup is pretty good.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment byuser comment-author-fredericmagnyf even thread-even depth-1 parent" id="comment-1256597">
				<div id="div-comment-1256597" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/Medinoc' rel='external nofollow' class='url'>Medinoc</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256597">
			July 19, 2016 at 7:22 am</a>		</div>

		<p>On the other hand, I did experience genuinely poor compression on Windows, using a .Net 2.0 System.IO.Compression.GZipStream to compress a TAR archive containing mostly JPEGs (i.e. already-compressed files); I did expect the resulting file to be slightly bigger than the original, but I expected *slightly* bigger, with an overhead in the 5%-10% range. Instead, the resulting tar.gz file was MORE THAN FIFTY PERCENT BIGGER than the original!</p>

		
				</div>
		<ol class="children">
		<li class="comment byuser comment-author-cheong odd alt depth-2 parent" id="comment-1256725">
				<div id="div-comment-1256725" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/cheong00' rel='external nofollow' class='url'>cheong00</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256725">
			July 19, 2016 at 7:10 pm</a>		</div>

		<p>Even ZIP or RAR can compress a file with greater size than original in it&#8217;s worst case, that&#8217;s why in implementing compression functions, you have to check the resulting file size with original, and if it&#8217;s larger, &#8220;compress&#8221; the file(s) again with &#8220;store only&#8221; in compression quality option.</p>

		
				</div>
		<ol class="children">
		<li class="comment byuser comment-author-fredericmagnyf even depth-3" id="comment-1256775">
				<div id="div-comment-1256775" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/Medinoc' rel='external nofollow' class='url'>Medinoc</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256775">
			July 20, 2016 at 4:39 am</a>		</div>

		<p>As I said, I expected that. I just didn&#8217;t expect it to be *that* bad, because even the most basic RLE compressor has a code to say &#8220;follow X bytes of unaltered data&#8221;!</p>
<p>I would have been OK with an output 10% bigger than the original. *Not* with an output 50% bigger.</p>

		
				</div>
		</li><!-- #comment-## -->
</ol><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment odd alt depth-2" id="comment-1256805">
				<div id="div-comment-1256805" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Joshua</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256805">
			July 20, 2016 at 7:14 am</a>		</div>

		<p>Don&#8217;t use the system copy of GZipStream; use the copy from ICSharpCode.SharpZipLib.dll</p>

		
				</div>
		</li><!-- #comment-## -->
</ol><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even thread-odd thread-alt depth-1 parent" id="comment-1256655">
				<div id="div-comment-1256655" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Kemp</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256655">
			July 19, 2016 at 10:02 am</a>		</div>

		<p>I wouldn&#8217;t expect them to be the same, though I&#8217;ve never had to articulate the reasoning behind that intuition. Makes perfect sense with a little thought. On the other hand, the decompression performance of the built-in Zip support is abysmal. *shakes fist at zip team*</p>

		
				</div>
		<ol class="children">
		<li class="comment odd alt depth-2" id="comment-1256755">
				<div id="div-comment-1256755" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Richard</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256755">
			July 19, 2016 at 11:51 pm</a>		</div>

		<p>I am pretty sure that the Windows built-in ZIP support is using a Schloem-the-painter algorithm for directory listing.</p>
<p>It is stupendously slow at merely navigating a ZIP containing a few thousand files &#8211; it can take almost a minute to do a simple directory listing.</p>
<p>It&#8217;s quite shocking that the QuaZip library can list *and* extract all the files in less time than Windows can drop one level in the directory tree &#8211; same hardware, same ZIP file.</p>

		
				</div>
		</li><!-- #comment-## -->
</ol><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment byuser comment-author-jader3rd even thread-even depth-1" id="comment-1256705">
				<div id="div-comment-1256705" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/jader3rd' rel='external nofollow' class='url'>jader3rd</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256705">
			July 19, 2016 at 4:20 pm</a>		</div>

		<p>Is there an API to call, to zip a file in the same manner how Windows Explorer zips a file?</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-1256745">
				<div id="div-comment-1256745" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Matteo Italia</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256745">
			July 19, 2016 at 11:21 pm</a>		</div>

		<p>TRWTF is that people in 2016 still install nagware like WinZip or WinRar to compress files.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment byuser comment-author-dwalker-wk even thread-even depth-1 parent" id="comment-1256785">
				<div id="div-comment-1256785" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/DWalker' rel='external nofollow' class='url'>DWalker</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256785">
			July 20, 2016 at 6:45 am</a>		</div>

		<p>I know that the file system needs to support &#8220;reading a file in the middle&#8221;; &#8220;reading the last byte&#8221;; etc.  But I&#8217;ll bet that 95% of file reads are from the start to the end.  </p>
<p>Surely, EXE files and DLLs are read from start to finish.  And Excel files, Word files, Powerpoint files&#8230; maybe not Access databases. </p>
<p>This doesn&#8217;t help the issue here.</p>

		
				</div>
		<ol class="children">
		<li class="comment byuser comment-author-fredericmagnyf odd alt depth-2" id="comment-1256815">
				<div id="div-comment-1256815" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/Medinoc' rel='external nofollow' class='url'>Medinoc</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256815">
			July 20, 2016 at 7:39 am</a>		</div>

		<p>Actually, for EXEs and DLLs I doubt it since memory-mapped sections are used: It&#8217;s likely parts would be paged on-demand (though I don&#8217;t know how prefetch stuff factors into this)&#8230;</p>

		
				</div>
		</li><!-- #comment-## -->
</ol><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even thread-odd thread-alt depth-1 parent" id="comment-1256865">
				<div id="div-comment-1256865" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">640k</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256865">
			July 20, 2016 at 11:03 am</a>		</div>

		<p>I always wished for real zipfile compression (call it transparent if you want) in a OS when storing files, with all expected drawbacks that would be more obvious, instead of some NIH &#8220;transparent&#8221; compression with leaking abstractions that doesn&#8217;t work well anyway, and usually contains a lot of black box surprises that in some cases works like &#8220;magic&#8221;.</p>

		
				</div>
		<ol class="children">
		<li class="comment byuser comment-author-fjeldse odd alt depth-2" id="comment-1256885">
				<div id="div-comment-1256885" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/Erik+F' rel='external nofollow' class='url'>Erik F</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256885">
			July 20, 2016 at 12:22 pm</a>		</div>

		<p>I am not a compression expert, but this is my take on why transparent file compression doesn&#8217;t use ZIP or other popular compression formats: archival compression doesn&#8217;t support modifications. The benefits of ZIP and friends are that they can take advantage of dictionaries to remove redundant data in the entire file, but by doing this it&#8217;s next to impossible to modify the resulting data stream. NTFS compression, on the other hand, essentially compresses in chunks of multiples of 8KB (see <a href="https://blogs.msdn.microsoft.com/ntdebugging/2008/05/20/understanding-ntfs-compression/" rel="nofollow">https://blogs.msdn.microsoft.com/ntdebugging/2008/05/20/understanding-ntfs-compression/</a>).</p>
<p>What would end up happening? Every write commit would require a brand-new data stream, so your single-byte modification could result in a multi-megabyte transaction (recomputing the dictionary and regenerating the compressed stream.) This does not seem like a very good idea.</p>
<p>As a read-only file system, I can&#8217;t see any problems with standard archival formats (that&#8217;s what they are internally!), but they&#8217;re not designed for and really shouldn&#8217;t be used as a R/W file system.</p>

		
				</div>
		</li><!-- #comment-## -->
</ol><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-1256905">
				<div id="div-comment-1256905" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">smf</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20160718-00/?p=93895#comment-1256905">
			July 20, 2016 at 12:46 pm</a>		</div>

		<p>&#8220;However, since the chunks are independent, they cannot take advantage of redundancy that is present in another chunk. (For example, if two chunks are identical, they still need to be compressed separately; the second chunk cannot say &#8220;I&#8217;m a copy of that chunk over there.&#8221;)&#8221;</p>
<p>Actually that shouldn&#8217;t be difficult to add, because you must have a central list of pointers to the compressed chunks. If two chunks were identical then you can just have two pointers. Writes become more complicated because you can&#8217;t necessarily overwrite a chunk if it changes. However writing is already complicated as chunks are variable length and you really don&#8217;t want to rewrite the entire file every time a byte changes. So it might not be that much more complicated.</p>
<p>A project I&#8217;m involved with did this because it needed seekable compression, our changes are stored in a separate file though. That way you can either merge the changes in, or delete them and go back to the original. Or even keep multiple sets of differences.</p>

		
				</div>
		</li><!-- #comment-## -->
		</ol><!-- .comment-list -->

		<div class="navigation pagination">
					</div>

	
			<p class="no-comments">Comments are closed.</p>
<!-- COMMENTS END -->
</div></td></tr></table>

</body>
</html>
<br/><div class="disclaimer">
*DISCLAIMER: I DO NOT OWN THIS CONTENT. If you are the owner and would like it removed, please
<a target="_blank" href="/contact.htm">contact me</a>.
The content herein is an archived reproduction of entries from
Raymond Chen's "Old New Thing" Blog (most recent link is <a target="_blank" href="https://devblogs.microsoft.com/oldnewthing/">here</a>).
It may have slight formatting modifications for consistency and to improve readability.
<br/><br/>
WHY DID I DUPLICATE THIS CONTENT HERE?
Let me first say this site has never had anything to sell and has never shown ads of any kind. I have nothing monetarily to gain by duplicating content here.
Because I had made my own local copy of this content throughout the years, for ease of using tools like grep, I decided to put it online after I discovered
some of the original content previously and publicly available, had disappeared approximately early to mid 2019.
At the same time, I present the content in an easily accessible theme-agnostic way.
<br/><br/>
The information provided by Raymond's blog is, for all practical purposes, more authoritative on Windows Development than Microsoft's
own MSDN documentation and should be considered supplemental reading to that documentation. The wealth of missing details
provided by this blog that Microsoft could not or did not document about Windows over the years is vital enough, many would agree an online "backup" of these details
is a necessary endeavor. Specifics include:<br/>
<ul>
    <li>
        A "redesign" after 2019 erased thousands of user's comments from previous years. As many have stated, the comments are nearly as important as the postings themselves.
        The archived copies of the postings contained here retain the original comments.
    </li>
    <li>
        The blog has changed domains many times and the urls have otherwise been under constant change since 2003.
        Even when proper redirection has been set up for those links, redirection only works for a limited period of time.
        For example, all of the internal blog links that were valid in early 2019, were broken by 2020 without proper redirection.
    </li>
    <li>
        The blog has been under constant re-design and re-theming since its inception.
        It is downright irritating to deal with a bogged-down site experience as the result of the latest visual themes designed for cell-phone browsers.
        As of this writing, it is cumbersome to navigate titles with only 10 entries per page.
        While it is nice that the official site has a search feature, searching using this index (with all titles on a single page) is much quicker (CTRL-F in most browsers).
    </li>
</ul>
</div><br/>
&lt;-- Back to <a href="index.htm">Old New Thing Archive Index</a>

