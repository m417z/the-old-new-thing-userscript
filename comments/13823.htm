<table class="commenttable" cellspacing="0" cellpadding="0"><tr><td><div class="commentdiv"><div class="commentdivhdr">
<!-- COMMENTS START -->
Comments (19)	</div>

	
			<div class="navigation pagination clear-both">
					</div>

		<ol class="comment-list">
					<li class="comment even thread-even depth-1" id="comment-837023">
				<div id="div-comment-837023" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Michael Mol</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20100603-00/?p=13823#comment-837023">
			June 3, 2010 at 7:42 am</a>		</div>

		<p>Geh. Module interface barriers. Time to put deprecate notices in the header file, put the new calls in, and replace the old calls with wrappers.</p>
<p>Wouldn&#39;t stop me from hearing complaints about build warnings for a while, but that problem tends to resolve itself.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-837043">
				<div id="div-comment-837043" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Gareth</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20100603-00/?p=13823#comment-837043">
			June 3, 2010 at 7:47 am</a>		</div>

		<p>When you say &quot;There is no multibyte-to-multibyte conversion function&quot;, don&#39;t you mean single-byte to single-byte conversion?</p>
<p>Also when you say &quot;Since one of the requirements for being an ANSI code page is that no single character can be more than 2 bytes&quot;, don&#39;t you mean more than 1 byte?</p>
<p>I&#39;m hoping these are mistakes because I thought I understood this subject; now I&#39;m not so sure.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-837083">
				<div id="div-comment-837083" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">laonianren</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20100603-00/?p=13823#comment-837083">
			June 3, 2010 at 8:14 am</a>		</div>

		<p>@Gareth. &nbsp;ANSI code pages can be either single-byte or multi-byte. &nbsp;For example, code page 1252 (used in the USA) uses one byte for each character, but code page 950 (traditional chinese) uses one or two bytes for each character.</p>
<p>Some code pages (e.g. code page 54936 &#8211; GB18030) can use more than 2 bytes per character but they can&#39;t be selected as &quot;the ANSI code page&quot; (i.e. the system default).</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-837093">
				<div id="div-comment-837093" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Tim</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20100603-00/?p=13823#comment-837093">
			June 3, 2010 at 8:34 am</a>		</div>

		<p>Assuming the questioner was using a specific single-byte code page (for example 1252) it is trivial to write a routine which converts an ANSI string directly to UTF-8. You need a lookup table containing the UTF-8 representations of the high-bit characters, and it&#39;s done.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-837123">
				<div id="div-comment-837123" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Ry Jones</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20100603-00/?p=13823#comment-837123">
			June 3, 2010 at 10:31 am</a>		</div>

		<p>You can convert the weather into unicode easily:</p>
<p><a rel="nofollow" target="_new" href="http://weather.mar.cx/" rel="nofollow">http://weather.mar.cx/</a></p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment byuser comment-author-maurits odd alt thread-odd thread-alt depth-1" id="comment-837153">
				<div id="div-comment-837153" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/Maurits+%5BMSFT%5D' rel='external nofollow' class='url'>Maurits [MSFT]</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20100603-00/?p=13823#comment-837153">
			June 3, 2010 at 12:21 pm</a>		</div>

		<p>For those who are reading this article and are confused by the idea of &quot;converting Unicode to UTF-8&quot;, I must point out that when a Microsoft employee says &quot;Unicode&quot; without further qualification, they mean UTF-16 LE.</p>
<p>e.g., &quot;Notead | Save As | Encoding&quot; contains these options:</p>
<p>ANSI</p>
<p>Unicode</p>
<p>Unicode big-endian</p>
<p>UTF-8</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-837173">
				<div id="div-comment-837173" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">David Walker</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20100603-00/?p=13823#comment-837173">
			June 3, 2010 at 12:35 pm</a>		</div>

		<p>The original questioner needs to understand what mathematicians call the &quot;pigeonhole principle&quot;.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-837193">
				<div id="div-comment-837193" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Yuhong Bao</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20100603-00/?p=13823#comment-837193">
			June 3, 2010 at 2:49 pm</a>		</div>

		<p>&quot;For those who are reading this article and are confused by the idea of &quot;converting Unicode to UTF-8&quot;, I must point out that when a Microsoft employee says &quot;Unicode&quot; without further qualification, they mean UTF-16 LE.&quot;</p>
<p>Yep, don&#39;t forget MS decided to adopt Unicode for NT before UTF-8 even existed!</p>
<p>Exercise: If UTF-8 was invented in 1992, why didn&#39;t NT 3.1 released in 1993 have UTF-8 support?</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-837203">
				<div id="div-comment-837203" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Marquess</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20100603-00/?p=13823#comment-837203">
			June 3, 2010 at 3:04 pm</a>		</div>

		<p>My answer to the original question would be something along the lines of “libiconv.“</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-837213">
				<div id="div-comment-837213" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Ben Hutchings</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20100603-00/?p=13823#comment-837213">
			June 3, 2010 at 4:24 pm</a>		</div>

		<p>Marquess: While some iconv implementations support arbitrary conversions, there is generally no requirement that they do so &#8211; and so far as I&#39;m aware, those that do support them convert via UTF-32 if it is neither the source nor destination encoding.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-837243">
				<div id="div-comment-837243" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Cheong</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20100603-00/?p=13823#comment-837243">
			June 3, 2010 at 6:10 pm</a>		</div>

		<p>Having a &quot;superset&quot; code page standing in the middle of conversion can greatly reduce the size of conversion library. (by reducing the pairs of codepages conversion table needed)</p>
<p>For Unicode conversions you have the additional benefit of cleaner handling of varies decomposition forms.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-837253">
				<div id="div-comment-837253" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">David</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20100603-00/?p=13823#comment-837253">
			June 3, 2010 at 8:10 pm</a>		</div>

		<p>Why didn&#39;t they write it themself? UTF-8 is not that hard.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-837263">
				<div id="div-comment-837263" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Anonymous Coward</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20100603-00/?p=13823#comment-837263">
			June 3, 2010 at 8:43 pm</a>		</div>

		<p>‘I would have to create a new Unicode interface, and modify all existing callers to switch to the new one.’ &#8211; From personal experience, that is easier than either dealing with lossy conversion issues or escaping.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-837293">
				<div id="div-comment-837293" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Worf</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20100603-00/?p=13823#comment-837293">
			June 3, 2010 at 11:04 pm</a>		</div>

		<p>UTF-8 is a compatibility version of Unicode &#8211; designed for 8-bit clean mediums but avoiding the one byte that can cause problems &#8211; NUL. UTF-16/32 are much easier to parse and handle, and you can get nice speed boosts by converting UTF-8 to UTF-16/32 ASAP if you know your medium can handle embedded NUL bytes.</p>
<p>Navigating UTF-8 is a pain also if you have to go backwards and forwards.</p>
<p>But, UTF-8 is great because most legacy systems can handle it with zero modifications &#8211; they don&#39;t have to be Unicode aware.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-837303">
				<div id="div-comment-837303" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Cheesle</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20100603-00/?p=13823#comment-837303">
			June 3, 2010 at 11:33 pm</a>		</div>

		<p>I really enjoy your stories, but I find this to be a bit ignorant:</p>
<p>&quot;Keep the poorly-designed ANSI interface around for backward compatibility, so that callers could switch to the Unicode-based interface at their leisure.&quot;</p>
<p>Why do you say poorly-designed? Has it occurred to you that the interface may have been designed 15 years ago, and at its time it was all up to the state of art, provided by its host OS at that time?</p>
<p>Old interfaces may not be easily replaced/duplicated, and in any case it depends on the availability of the source.</p>
<p>Just because something may be old, it is not necessarily poorly-designed.</p>
<p>As for UTF-8 being great, as stated by Worf&#8230; For English yes, once you go beyond 1252, UTF-8 is not great.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-837423">
				<div id="div-comment-837423" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">mdw</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20100603-00/?p=13823#comment-837423">
			June 4, 2010 at 9:19 am</a>		</div>

		<p>@ Worf &quot;UTF-8 is a compatibility version of Unicode &#8211; designed for 8-bit clean mediums but avoiding the one byte that can cause problems &#8211; NUL. UTF-16/32 are much easier to parse and handle, and you can get nice speed boosts by converting UTF-8 to UTF-16/32 ASAP if you know your medium can handle embedded NUL bytes.&quot;</p>
<p>UTF-8 doesn&#39;t avoid NUL bytes at all. &nbsp;Are you thinking of Sun&#39;s demented UTF-8 variant?</p>
<p>For a new application, UTF-16 has only one discernible advantage over UTF-8: it&#39;s more compact at representing characters from some Asian languages (two bytes per character rather than three). &nbsp;UTF-16 has the same variable-length encoding problems that UTF-8 has, only they happen more rarely so your code gets tested less well; but it doesn&#39;t have UTF-8&#39;s compatibility with old 8-bit string functions. &nbsp;This goes much further than you might think: UTF-8 strings order lexicographically exactly as the corresponding sequences of code-points do, for example. &nbsp;This is /not/ true of UTF-16, because characters outside the BMP are encoded with surrogate pairs, and the surrogate space isn&#39;t at the top of the BMP.</p>
<p>Not all text processing jobs are faster on UTF-32. &nbsp;UTF-8 has a compactness advantage &#8212; and therefore a locality advantage &#8212; on things that can be done in a single left-to-right pass (or a few of them). &nbsp;There are other advantages to UTF-8 too. &nbsp;Perl is no slouch at text processing; it uses UTF-8 as its internal representation. &nbsp;I&#39;m willing to bet that a good reason for this is that a lot of what it does is searching for substrings. &nbsp;It uses the Boyer&#8211;Moore algorithm, which scans the needle string and builds a table: if I&#39;m comparing with this character in the needle and I find this other character instead, I can skip over so many haystack characters because there&#39;s no hope of a match there. &nbsp;In UTF-32 the tables would be enormous. &nbsp;In UTF-8, it&#39;s still just 256 bytes per character position &#8212; and the search works fine on full Unicode. &nbsp;Other text-processing algorithms &#8212; e.g., lexical analysers &#8212; which work by building and running a DFA need fancy table compression techniques if you&#39;re going to use UTF-32 (and handling the intermediate forms is probably awful). &nbsp;In UTF-8, your DFA ends up being a little more complicated at the beginning but everything else is fairly tractable &#8212; and you can avoid the extra indirections from DFA table decompression.</p>
<p>That&#39;s not to say that having a fixed-size per code-point isn&#39;t good for other jobs. &nbsp;Yes, if you want to pick out the substring between characters 5 and 17 then UTF-8 sucks. &nbsp;But for fixed size per code-point, UTF-32 is the only way to fly.</p>
<p>Windows is stuck with UTF-16 because Unicode expanded after Microsoft had already decided to use two bytes per code-point, and UTF-16 is the compatibility path from UCS-2. &nbsp;If they were starting now, I&#39;d bet they&#39;d choose UTF-8 rather than UTF-32, and UTF-16 wouldn&#39;t even have a chance.</p>
<p>I think UTF-8 was a better choice from the beginning. &nbsp;But at the time it was a sketch on Ken Thompson&#39;s napkin, for use in Plan 9 from Bell Labs. &nbsp;I don&#39;t blame MS for not inventing UTF-8 themselves. &nbsp;Firstly, Ken Thompson is a genius, and secondly he was designing a research-toy OS at the time. &nbsp;UCS-2 was already specified; Microsoft took the conservative route, and it looked like a plausible choice at the time. &nbsp;After all, Unicode was probably seen as somewhat risky at the time anyway; inventing a proprietary encoding risked being stuck with unpleasant interoperability problems (and accusations about undermining industry standards and all that, probably). &nbsp;Does that answer Yuhong Bao&#39;s exercise question?</p>
<p>I /do/ blame MS for producing a shiny new runtime system that calls a 16-bit quantity a `char&#39; in 2002; it&#39;s just a lie. &nbsp;They missed opportunity to leave behind what (in retrospect) turned out to be a mistake. &nbsp;At least they chose the default I/O encoding right.</p>
<p>Pre-emptive reply. &nbsp;The internal representation inside a <code>String&amp;#39; object is, or should be, opaque anyway. &amp;nbsp;If keeping it in UTF-16, or inventing a UTF-16 copy on demand, speeds up the FFI or anything else, then it can do that and nobody needs to care (benefits of a high-level RTS). &amp;nbsp;I don&amp;#39;t care about</code>String&#39;; I care about <code>char&amp;#39;. &amp;nbsp;Personally, I think making</code>char&#39; an integer type at all was a mistake: Common Lisp has adapted to Unicode with hardly a hitch, because it had an abstract `character&#39; type. &nbsp;I&#39;m guessing they just followed Java&#39;s lead on that one. &nbsp;But now I&#39;m /seriously/ risking getting Raymond annoyed with me, so I&#39;ll shut up now.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-837513">
				<div id="div-comment-837513" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Yuhong Bao</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20100603-00/?p=13823#comment-837513">
			June 4, 2010 at 10:05 pm</a>		</div>

		<p>mdw: I was not suggesting that MS would have invented UTF-8 themselves when I asked this question. I agree that it would have not been a good idea.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-837623">
				<div id="div-comment-837623" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Dude</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20100603-00/?p=13823#comment-837623">
			June 6, 2010 at 6:47 pm</a>		</div>

		<p>Arrays of UTF8 chars (aka string) is a pita.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-837633">
				<div id="div-comment-837633" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Dude</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20100603-00/?p=13823#comment-837633">
			June 6, 2010 at 6:47 pm</a>		</div>

		<p>Arrays of UTF8 chars (aka string) is a pita.</p>

		
				</div>
		</li><!-- #comment-## -->
		</ol><!-- .comment-list -->

		<div class="navigation pagination">
					</div>

	
			<p class="no-comments">Comments are closed.</p>
<!-- COMMENTS END -->
</div></td></tr></table>