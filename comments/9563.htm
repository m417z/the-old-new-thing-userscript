<table class="commenttable" cellspacing="0" cellpadding="0"><tr><td><div class="commentdiv"><div class="commentdivhdr">
<!-- COMMENTS START -->
Comments (36)	</div>

	
			<div class="navigation pagination clear-both">
					</div>

		<ol class="comment-list">
					<li class="comment even thread-even depth-1" id="comment-936883">
				<div id="div-comment-936883" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">David Walker</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-936883">
			September 23, 2011 at 8:18 am</a>		</div>

		<p>The fact that I/O to compressed partitions are completed synchronously is one of the reasons that data files which contain SQL databases should not be on compressed partitions. &nbsp;SQL server really, really wants its I/O to be asynchronous. &nbsp;And the performance of SQL server with fully synchronous I/O would likely be poor.</p>
<p>In the old days, before compressed SQL backups, I tried to write a SQL backup to a compressed partition. &nbsp;It didn&#39;t work (delayed I/O failed).</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-936903">
				<div id="div-comment-936903" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Dave</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-936903">
			September 23, 2011 at 9:02 am</a>		</div>

		<p>From that KB article: &quot;&#8230; because the pool of worker threads is limited (currently three on a 16MB system) &#8230;&quot;</p>
<p>Ah, 16MB systems, now those were the days! Wait, no they weren&#39;t.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-936913">
				<div id="div-comment-936913" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Gabe</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-936913">
			September 23, 2011 at 10:16 am</a>		</div>

		<p>Does anybody know why these operations are not completed asynchronously?</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-936923">
				<div id="div-comment-936923" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Leo Davidson</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-936923">
			September 23, 2011 at 10:18 am</a>		</div>

		<p>So if you open a new, empty file and use async I/O to write data into it, all of that will become synchronous?</p>
<p>Async I/O seems so caveat-ridden, and almost impossible to get every possibility handled correctly (from previous posts on the subject), that I&#39;m more and more convinced it&#39;s usually better to just spawn a thread (or pool) and use sync I/O.</p>
<p>Of course, that doesn&#39;t apply to every situation (and most situations don&#39;t need async at all), but I&#39;d rather have slightly higher overheads and correct code than incorrect code.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-936933">
				<div id="div-comment-936933" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Simon Farnsworth</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-936933">
			September 23, 2011 at 10:48 am</a>		</div>

		<p>@Gabe:</p>
<p>I can make some educated guesses. For compression and encryption, I would guess that the filesystem driver is simply unable to handle more than one outstanding request at a time for any one file, so going asynchronous has high costs (locking issues) and no benefits. Similar reasoning applies to extending a file, plus the additional factor that the intended consumers of async I/O typically don&#39;t extend files.</p>
<p>Data in cache is similarly obvious &#8211; giving you a mapping to the data in cache immediately is cheaper than handling the async completion &#8211; why pay a performance penalty when the point of async is to speed things up?</p>
<p>Finally, the data not in cache makes sense if you see async as a way to issue multiple requests to the hardware (which can normally handle multiple requests in parallel with a performance boost &#8211; SCSI TCQ, SATA NCQ, RAID arrays etc). No point going async if you&#39;re not getting the parallelism &#8211; Windows might as well block you as spawn a new thread that gets immediately blocked anyway.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-936943">
				<div id="div-comment-936943" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Simon Farnsworth</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-936943">
			September 23, 2011 at 10:52 am</a>		</div>

		<p>@Leo Davidson:</p>
<p>It looks like Windows async I/O is meant to let you take advantage of hardware parallelism, not to act as a true async mechanism. A SCSI TCQ device can potentially handle millions of simultaneous outstanding requests (e.g. a FC-AL attached RAID array with terabytes of cache RAM and petabytes of spinning disk). Async I/O lets you issue multiple I/O requests from one thread, as long as the software can extract parallelism from them (i.e. convert them into multiple requests to hardware); as soon as it has to sequence things, it goes synchronous, as there&#39;s no benefit to multiple outstanding requests if they must be handled one at a time.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-936953">
				<div id="div-comment-936953" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='http://blogs.msdn.com/jas71_4000_hotmail.com/ProfileUrlRedirect.ashx' rel='external nofollow' class='url'>jas71@hotmail.com</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-936953">
			September 23, 2011 at 1:03 pm</a>		</div>

		<p>@Leo Davidson: That&#39;s the scenario I had in mind: I can easily imagine a developer getting bitten by this, using overlapped IO to log activity thinking this will minimize the performance impact of log writes, when in fact every write will be converted to a synchronous one. I suspect it will actually only happen on writes which cross a cluster boundary, in reality (NTFS will be zero-filling each cluster as you start writing into it; overlapped writes within a cluster already in use won&#39;t have that issue). For that matter, storing your log files compressed seems an &quot;obvious&quot; improvement many system administrators might well make &#8211; then wonder why their server application is much less responsive, since it has suddenly had all its logging operations converted into synchronous ones &#8211; and the developer had carefully worked around that issue by slotting in occasional SetFileValidData calls to amortize the file expansion by using 1 or 10 Mb chunks and reduce file fragmentation at the same time.</p>
<p>The virtual memory system faced the same issue, but addressed it by pre-zeroing a pool of pages in the background for later use. Presumably it&#39;s the need for metadata updates (volume allocation bitmap changes, journaling and MFT record changes) which made the NTFS developers make write-extends synchronous &#8211; ironic in a way, since the extra time taken would make that particular operation an obvious candidate for asynchronous operation. With such a limited pool of threads, though, like the 3 mentioned, writes would sometimes tie up those precious resources for a while as the disk buffer is flushed (to ensure on-disk consistency with the journal).</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-936983">
				<div id="div-comment-936983" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Alex Grigoriev</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-936983">
			September 23, 2011 at 3:19 pm</a>		</div>

		<p>There is a good reason for serialization of these operations.</p>
<ol>
<li>
<p>All metadata modifications are serialized. File expansion needs metadata modification. While the file zeroing goes forward, all other IO to that area need to be held, too.</p>
</li>
<li>
<p>Creating new isle in a sparse file also needs metadata modification.</p>
</li>
<li>
<p>Compressed file writes need to be serialized because they produce unknown amount of hard data and need filesystem modifications.</p>
</li>
</ol>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-936993">
				<div id="div-comment-936993" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Gabe</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-936993">
			September 23, 2011 at 3:40 pm</a>		</div>

		<p>Why would the need to serialize the operations mean that they have to be synchronous? I don&#39;t see the difference between a single thread with multiple simultaneous outstanding async operations and multiple threads with simultaneous sync operations.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-937013">
				<div id="div-comment-937013" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">waleri</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937013">
			September 24, 2011 at 12:32 am</a>		</div>

		<p>What will happen with completion port whern async I/O is converted to sync?</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-937023">
				<div id="div-comment-937023" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Simon Farnsworth</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937023">
			September 24, 2011 at 3:49 am</a>		</div>

		<p>@Gabe:</p>
<p>Been thinking about this overnight (yes, I&#39;m dull). If you see the async operations as a way to cheaply exploit command queueing, it makes sense. When the OS can convert an operation directly into a disk request, all it needs do is remember &quot;when this I/O request completes on the disk, signal this completion&quot;; it has to have something like that internally anyway to permit it to block one thread, execute another, and resume the first thread once its I/O has completed.</p>
<p>Adding asynchronous handling of serialized requests means adding fresh complexity &#8211; you need to spawn a thread, and ensure that the thread you&#39;ve spawned simply runs the request synchronously and then signals the completion. However, you&#39;ve now moved away from your &quot;cheap method to exploit hardware parallelism&quot;; you&#39;re paying the costs of spawning and destroying a thread for operations. Far easier to just not bother &#8211; if you wanted true asynchronous behaviour, you&#39;d spawn threads anyway; without this async I/O interface, there&#39;s no way to say &quot;if the hardware lets me issue parallel requests, I want to be asynchronous; if I&#39;d only be exploiting software parallelism, I want to free up resources for CPU-bound processing&quot;.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-937033">
				<div id="div-comment-937033" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">JM</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937033">
			September 24, 2011 at 8:27 am</a>		</div>

		<p>There is only a problem (from a developer POV) if operations that take a significant amount of time are performed synchronously when you wanted them to be performed asynchronously. A cache write that immediately completes is fine. There is no parallelism to exploit because the operation takes no time; you can simply go on and do whatever you intended to do while the I/O was underway. But a disk operation that completes in milliseconds is an eternity to unexpectedly block, especially if you intended to do something non-I/O related inbetween. This is not usually a problem because asynchronous I/O is usually mixed with yet more asynchronous I/O, not calculations that have nothing to do with keeping I/O flowing, which are handled by other threads.</p>
<p>As Simon points out, although the OS could jump through hoops to make sure an async operation is always async, this doesn&#39;t pay off. Making an inherently synchronous operation asynchronous requires overhead (although not as dire as creating and destroying threads &#8212; you can maintain a thread pool for it). It&#39;s intended to save resources, not to support a particular programming model. It&#39;s up to the programmers to exploit it meaningfully. If necessary, you can always layer forced asynchronous completion over synchronous operations yourself with use of the thread pool &#8212; but since you can&#39;t reliably predict when synchronous processing will kick in, this means all operations will have to go through an extra layer. It&#39;s better to write your code in such a way that it&#39;s still correct in every way (especially with regards to timing and responsiveness) even if all I/O were synchronous, and treat asynchronous I/O as a good optimization.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-937063">
				<div id="div-comment-937063" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Gabe</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937063">
			September 26, 2011 at 5:52 am</a>		</div>

		<p>In the case of writing to a compressed file I can understand not spawning a new thread to perform compression. But once the data has been compressed, why not make the actual writing of the data asynchronous? At that point the CPU work has been completed and all that&#39;s left is the I/O. Does that I/O still need to be done synchronously?</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-937233">
				<div id="div-comment-937233" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Confused Developer</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937233">
			September 26, 2011 at 2:08 pm</a>		</div>

		<p>So then what is the right way to handle overlapped I/O to files on NTFS to ensure that your limited I/O threads are never blocked waiting for platters?</p>
<p>If the operating system itself had done the right thing and implemented serializing under the hood (while preserving async interface/behavior) it would have resulted in less error prone software and less need for each developer to reinvent the wheel.</p>
<p>This is the first time I&#39;ve become aware of this and I can imagine there must be a lot of code out there that expects Windows to always complete I/O calls asynchronously if they will not complete immediately. All that code will probably cause unintended blocking/performance problems if used against compressed volumes.</p>
<p>I think this is a bad implementation decision on the part of MSFT.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment byuser comment-author-cheong even thread-even depth-1" id="comment-937253">
				<div id="div-comment-937253" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/cheong00' rel='external nofollow' class='url'>cheong00</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937253">
			September 26, 2011 at 6:46 pm</a>		</div>

		<p>@Confused Developer: Why do you think it is so important? I don&#39;t think it&#39;s right to store frequently updating files in compressed folder. If they do, the performance penalty is expected (even though it is greater than absolute minimum).</p>
<p>Think about it, since Windows does not support CPU quota, if your little program continuously throwing &quot;little write to large compressed file&quot; to kernal I/O process, it would have been easy to create some form of DOS attack. (Remember NTFS compression is not in chunked format, so even altering 1 byte might require to decompress large part or even the whole file, alter the byte, and compress it back. It could be a quite expensive task.)</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment byuser comment-author-cheong odd alt thread-odd thread-alt depth-1" id="comment-937353">
				<div id="div-comment-937353" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/cheong00' rel='external nofollow' class='url'>cheong00</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937353">
			September 26, 2011 at 11:52 pm</a>		</div>

		<p>@Crescens2k: Ah&#8230; you&#39;re right. NTFS compress file into compression unit hence it&#39;s chunked. I must have mixed it up with something else.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-937363">
				<div id="div-comment-937363" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Deduplicator</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937363">
			September 27, 2011 at 12:14 am</a>		</div>

		<p>Crescens2k:&quot;But async IO is really there to allow you to cheaply exploit what is available in hardware. If that isn&#39;t possible because it requires multiple operations then Windows wont do it. If you look at some other operations which are never async then this should be apparent. There is a major difference between adding to a queue in the IO manager and having to create a thread to possibly do multiple file operations and all of the related work after all.&quot;</p>
<p>@^: Does not look like Windows would have to create any new threads for that work. It could all be done on only one thread per core, possibly inheriting the priority of the highest outstanding request.</p>
<p>Aside from that, it is often NOT used to exploit hardware parallelization, but to make the Server/GUI/whatever responsive, and that is quite more difficult if you cannot know if Windows forces you synchronous, instead of serializing behind the scenes and out of your thread in kernel-land.</p>
<p>Next: One has to beware unintended deep recursion when requests return immediately and are acted upon by invoking the handler.</p>
<p>chaong00: &quot;Think about it, since Windows does not support CPU quota, if your little program continuously throwing &#39;little write to large compressed file&#39; to kernal I/O process, it would have been easy to create some form of DOS attack&quot;</p>
<p>@^: Well, so all the work has to be debited from the originators ledger? That should be done anyway!</p>
<p>JM: &quot;There is only a problem (from a developer POV) if operations that take a significant amount of time are performed synchronously when you wanted them to be performed asynchronously. [&#8230;] This is not usually a problem because asynchronous I/O is usually mixed with yet more asynchronous I/O, not calculations that have nothing to do with keeping I/O flowing, which are handled by other threads.</p>
<p>As Simon points out, although the OS could jump through hoops to make sure an async operation is always async, this doesn&#39;t pay off. Making an inherently synchronous operation asynchronous requires overhead (although not as dire as creating and destroying threads &#8212; you can maintain a thread pool for it). It&#39;s intended to save resources, not to support a particular programming model. It&#39;s up to the programmers to exploit it meaningfully. If necessary, you can always layer forced asynchronous completion over synchronous operations yourself with use of the thread pool &#8212; but since you can&#39;t reliably predict when synchronous processing will kick in, this means all operations will have to go through an extra layer. It&#39;s better to write your code in such a way that it&#39;s still correct in every way (especially with regards to timing and responsiveness) even if all I/O were synchronous, and treat asynchronous I/O as a good optimization.&quot;</p>
<p>@^: There are often two or more different limiting factors, so there is a problem, even if all is ordered asynchronously: e.g. File IO, Remote File IO 1..N, Vanilla Networking, Calculating, GUI, &#8230;</p>
<p>And the programmer might not (be able to) know, what all is involved.</p>
<p>And its nice that you celebrate working around the quagmire which should not be there as prudent even in the absence of said hazard.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-937373">
				<div id="div-comment-937373" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Simon Farnsworth</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937373">
			September 27, 2011 at 1:05 am</a>		</div>

		<p>@Deduplicator:</p>
<p>Look at it this way; if you use synchronous I/O and threads, you never block (what you want for responsiveness). If you want to use synchronous I/O and threads to exploit the inherent parallelism in a big RAID array (say a nice EMC SAN), you need tens of thousands of threads, which literally exist just to issue an I/O asynchronously to the main thread.</p>
<p>Async I/O lets you get at a middle ground that&#39;s otherwise unavailable &#8211; the statement you make by using async I/O is &quot;if there is hardware parallelism available, let me at it; otherwise, I&#39;m happy going synchronous&quot;. Remember, it&#39;s designed for services like SQL Server; the pattern it&#39;s best at is &quot;lots of I/O so that I can process for a bit, then back to lots of I/O&quot;. It&#39;s just that it issues all the I/O in parallel where possible, getting you a free speed-up as compared to synchronous I/O.</p>
<p>Think, for example, about a small RAID-1 array with 2 disks in it; that array is guaranteed to be able to handle two simultaneous reads without conflict, as it has two spindles; a RAID-10 with 4 disks may be able to handle two reads and two writes (depending on disk locations) simultaneously. Bigger arrays with more spindles can handle more IOPs in parallel. Async I/O lets you say &quot;please issue lots of I/O if possible; I&#39;ll handle collating it back together later&quot;.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-937383">
				<div id="div-comment-937383" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Deduplicator</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937383">
			September 27, 2011 at 2:21 am</a>		</div>

		<p>@Simon:</p>
<p>So you are saying, it&#39;s nice you have to go needlessly multithreaded and accept all the gotchas involved, even though everything could be easily done using guaranteed user-level async calls?</p>
<p>Damn anyone who uses async as it reads (and works most of the time) to the pit of spuriously non-responsive and subtly broken code? (ie: the WinXP-Explorer Guys and anyone else who uses the Shell-Namespace [GetOpenFileName anyone?])</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-937393">
				<div id="div-comment-937393" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Simon Farnsworth</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937393">
			September 27, 2011 at 3:21 am</a>		</div>

		<p>@Deduplicator</p>
<p>The only way to implement &quot;guaranteed user-level async calls&quot; is to go multithreaded and accept all the gotchas involved. Whether the OS provides this as a standard library, or whether you implement it yourself, the problems are exactly the same.</p>
<p>As a result, you could write yourself a library that spawns off a small number of threads that just do I/O, working on a queue of requests from the main thread, and queueing responses back to the main thread. This would be identical to what you&#39;re asking for.</p>
<p>However, if the async I/O interface becomes that library, you have two problems to fix:</p>
<ol>
<li>Older code that uses async I/O may not be expecting it to suddenly make your process multithreaded, and could therefore be caught out by the gotchas you mention.</li>
<li>
<p>There is now no way for a process to request I/O to be done asynchronously if and only if the hardware can handle the parallelism for you &#8211; any async I/O request that can&#39;t be parallelised in hardware converts to a thread handling the request, so you can&#39;t use the async I/O facility to issue parallel I/O, blocking whenever you&#39;re reached the hardware parallelism limit for this system.</p>
</li>
</ol>
<p>In short, why do you want to take away a useful facility for no gain to anyone?</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-937303">
				<div id="div-comment-937303" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Crescens2k</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937303">
			September 26, 2011 at 9:18 pm</a>		</div>

		<p>Confused Developer:</p>
<p>The biggest issue is that with NTFS compression, you can&#39;t guarantee that one IO operation will really result in one IO operation.</p>
<p>NTFS uses a form of LZ compression, and that means it does the compression in chunks of data. From what I read somewhere, it implements it so that it takes a few clusters (I think it is 16 clusters), compresses it and then stores it compressed in those cluster leaving the gap between the end of the compressed data and the next chunk as sparse. So it could turn out like</p>
<p>uncompressed</p>
<p>|data&#8230;.|data&#8230;.|data&#8230;.|</p>
<p>compressed</p>
<p>|cdata.. |cd.. &nbsp; &nbsp;|cdata &nbsp; |</p>
<p>Where the spaces are sparse and just not in use. For reads this means two things. If you just want to read part of the file, you have to read the entire chunk to get the data you want. If the data you want spans two chunks, then you need two seperate read/decompress operations to do what you want. This is different to the uncompressed file since if you only want to read 2 bytes then it would only have to read a sector off of the disk, and if what you wanted spanned two sectors, you could read both in a single operation. (This is ignoring some hard disk hardware limitations though, but they just confuse things).</p>
<p>For writes this gets worse, since if you wanted to update only a couple of bytes, then you would need to read the entire data chunk, decompress, update, compress and then write. If the data spans more than one chunk then it needs to do this for each chunk affected.</p>
<p>So yes, while making this kind of operation may seem like a performance hit, allowing compressed file operations to be async doesn&#39;t fix anything. Async works because Windows just exploits what is available, but for operations like compressed file operations, everything changes drastically. It is no longer the case of filling out a buffer and queuing it up in the IO manager, it becomes an entire thread of work. Windows is bloated enough as it is, do you want even more. You could also run into other errors because compressed file IO is much slower than regular IO, so you could end up running out of memory because the queue got too long or something. So all you managed to do is displace the problems.</p>
<p>I am also curious as to how allowing async compressed file operations would make code less error prone. A normal async file write could be</p>
<p>if(!WriteFile(/*parameters*/)) //returns nonzero if completed immediately</p>
<p>{</p>
<p> &nbsp; &nbsp;if(GetLastError() != ERROR_IO_PENDING)</p>
<p> &nbsp; &nbsp;{</p>
<p> &nbsp; &nbsp; &nbsp; &nbsp;//i personally use a switch here, adding cases</p>
<p> &nbsp; &nbsp; &nbsp; &nbsp;//only for errors that need special attention, but</p>
<p> &nbsp; &nbsp; &nbsp; &nbsp;//usually leaving everything go to default</p>
<p> &nbsp; &nbsp; &nbsp; &nbsp;//an error like insufficient disk space occured</p>
<p> &nbsp; &nbsp; &nbsp; &nbsp;//handle these cases</p>
<p> &nbsp; &nbsp;}</p>
<p>}</p>
<p>Because any other error can occur and get returned via the WriteFile return value, you can&#39;t skip the check (even though people do seem to assume that file IO can&#39;t fail), so regardless you will be checking the return. So by an operation being async instead just means that the function will return an error an you have to look for it, or if it isn&#39;t async then it means the function will block and return success. So how does this make anything less error prone? Nothing would have changed from your regular async file handling.</p>
<p>But async IO is really there to allow you to cheaply exploit what is available in hardware. If that isn&#39;t possible because it requires multiple operations then Windows wont do it. If you look at some other operations which are never async then this should be apparent. There is a major difference between adding to a queue in the IO manager and having to create a thread to possibly do multiple file operations and all of the related work after all.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-937343">
				<div id="div-comment-937343" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">pattern</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937343">
			September 26, 2011 at 11:49 pm</a>		</div>

		<p>How about this pattern?</p>
<p>SetLastError(0);</p>
<p>WriteFile(/*Params*/);</p>
<p>switch(GetLastError){</p>
<p>default: // All the myriad other error codes&#8230; Maybe single out some for special processing</p>
<p>FatalAppExit(0,&quot;Bang!&quot;);</p>
<p>abort();</p>
<p>case 0: PostQueuedCompletionStatus(/*params*/) or QueueUserAPC(/*params*/)</p>
<p>case ERROR_IO_PENDING:</p>
<p>}</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-937513">
				<div id="div-comment-937513" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Deduplicator</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937513">
			September 27, 2011 at 3:42 pm</a>		</div>

		<p>The only way to implement &quot;guaranteed user-level async calls&quot; in user-mode on top of maybe async and/or explicit sync calls is certainly going multithreaded. But that&#39;s not what I asked for. What I asked was why there is no way to have really async calls without going multithreaded. I haven&#39;t seen an answer, which doesn&#39;t say go multithreaded or tough luck, that&#39;s the way it is, therefore it&#39;s good.</p>
<p>Also, its extremely counter-intuitive that non-blocking methods block for minutes or even longer.</p>
<div class="post">[<i>We&#39;ve seen earlier that it&#39;s possible for async calls to complete synchronously, even if everything supports asynchronous I/O, so you have to be ready for it one way or another. (And if you avoid using overlapped writes to extend files, that avoids the worst case.) -Raymond</i>]</div>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-937553">
				<div id="div-comment-937553" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Gabe</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937553">
			September 27, 2011 at 9:47 pm</a>		</div>

		<p>The problem isn&#39;t when your async call completes synchronously (as that happens all the time anyway, so it has to be something you expect), it&#39;s when you make async I/O calls on your UI thread because you expect async I/O not to block. The fact that async calls can actually block means that you can&#39;t keep your UI responsive simply by making all your I/O async; you have to make sure you never do any I/O on your UI thread. There being no async CreateFile should be the tip-off for this, but still many people don&#39;t know this.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-937603">
				<div id="div-comment-937603" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Simon Farnsworth</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937603">
			September 28, 2011 at 6:03 am</a>		</div>

		<p>@Deuplicator:</p>
<p>So, very roughly (as I&#39;m not a Microsoft developer), the reasoning works as follows:</p>
<p>Async I/O avoids the need for threads by exploiting the hardware&#39;s queueing. When you call a synchronous I/O call that could complete async, the following happens:</p>
<p>1) The kernel does the work needed to know what commands it needs to send to hardware</p>
<p>2) It flags your thread as &quot;blocked, waiting for I/O&quot; for scheduling purposes</p>
<p>3) It creates a completion that tells the scheduler to unblock your thread</p>
<p>4) It issues the commands (blocking here for as long as it takes for there to be space in the hardware queue) with the completion set up ready to be run by the interrupt handler</p>
<p>5) It enters the scheduler, which finds something to run.</p>
<p>You recover control when the I/O completes.</p>
<p>If you used an async call instead, the completion created in step 3 changes from &quot;tell the scheduler to unblock this thread and reschedule&quot; to &quot;mark this OVERLAPPED as completed and flag this thread as having a completion to handle when it calls back in to find one&quot;. As a result, step 5 allows the scheduler to decide to continue running your thread. Remember that because the completion is called by an interrupt handler, it can&#39;t do much &#8211; it can write flags, and it can kick the scheduler, and that&#39;s about it &#8211; it can&#39;t start a new thread, it can&#39;t do complex work, and it can&#39;t rearrange your thread&#39;s execution to suddenly drop into kernel code unexpectedly.</p>
<p>For calls that go synchronous even when issued via the async I/O mechanism, the kernel has work to do after the hardware completes. The process then looks like:</p>
<p>1) The kernel does the work needed to know what commands it needs to send to hardware</p>
<p>2) It flags your thread as &quot;blocked, waiting for I/O&quot; for scheduling purposes</p>
<p>3) It creates a completion that tells the scheduler to unblock your thread</p>
<p>4) It issues the commands (blocking here for as long as it takes for there to be space in the hardware queue) with the completion set up ready to be run by the interrupt handler</p>
<p>5) It enters the scheduler, which runs another thread.</p>
<p>6) When the I/O completes, the kernel does the clean-up work it needs to do to handle this I/O.</p>
<p>7) It finally returns control to your thread.</p>
<p>Note that there is no way for the kernel to do work on your process&#39;s behalf without either requiring you to cope with the fallout of multiple threads (as it would potentially create a thread for you at step 3, which would be unblocked when the I/O completes so that it can execute step 6 and then signal completion to your thread), or blocking you until it&#39;s completed the work it needs to do. Given that the point of async I/O is to get you parallelism without threads, it can&#39;t create a thread, so the only remaining option is to block you.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-937613">
				<div id="div-comment-937613" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Joe</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937613">
			September 28, 2011 at 7:52 am</a>		</div>

		<p>&quot;We&#39;ve seen earlier that it&#39;s possible for async calls to complete synchronously, even if everything supports asynchronous I/O, so you have to be ready for it one way or another.&quot;</p>
<p>That some calls on a overlapped handle can already be completed on return from that call does not imply that &quot;conversions&quot; from async to sync mode do occur. I would only take this as a sign that (for example) all the data requested by a ReadFile call is already present in the buffer cache of the OS and therefore the call can do his work without any wait.</p>
<p>Saying now here that the semantics of API calls in regards of blocking can change on the fly due to inadequate implementation of some detail at some I/O layer deep below this API call (you really cannot know such implementations details as an application programmer) looks flawed.</p>
<div class="post">[<i>But if you think about it, the two types of synchronous behavior are identical. They just have different performance characteristics. The I/O manager calls the handler&#39;s &quot;StartAsync&quot; function and the handler returns &quot;Done. Oh, and by the way, it&#39;s also finished.&quot; The deal is that the &quot;Oh, and by the way it&#39;s also finished&quot; could be because &quot;Oh, the answer was so easy to calculate I just did it since it was less work than setting up a full async I/O&quot;. Or it could be because &quot;Oh, it&#39;s too hard for me to set up the async I/O for this, so I&#39;ll just do all the work inside my StartAsync function and say &#39;finished&#39;.&quot; -Raymond</i>]</div>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-937673">
				<div id="div-comment-937673" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Simon Farnsworth</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937673">
			September 28, 2011 at 8:46 am</a>		</div>

		<p>@Joe:</p>
<p>The MSDN docs already say that conversions from async to sync can occur, and point you to a Knowledge Base article that details exactly when they occur for specific OSes (i.e. what the implementation details are). Async I/O as documented on MSDN is strictly a performance boost, not a guarantee of non-blocking.</p>
<p>If you&#39;re using an API call based on what you would like it to be, not what it&#39;s documented to be, well, you&#39;re going to get burnt sooner or later.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-937723">
				<div id="div-comment-937723" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Deduplicator</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937723">
			September 28, 2011 at 10:29 am</a>		</div>

		<p>@Simon (long post): Ever heard about kernel-mode APCs? There&#39;s your way to schedule work after the pure IO is done, without creating new threads, and especially user-mode threads.</p>
<p>@Simon+Gabe: The fact that async is not guaranteed is no reason it should not be or could not be!</p>
<p>@raymond: you don&#39;t seriously propose that compression/decompression, encrypting/decrypting or extending a file or some such operation is so complex, it has to be handled on the kernel-mode part of the exact same user-mode thread, and additionally no part of it (the writing part for example) can actually be handed of to the IO-Subsystem to do its thing without supervision by the blocked thread hovering impatiently in the background? And arguing that behavior is the same, whether you instantly get the go ahead or have to wait the year is a bit strange.</p>
<p>Simon (short post): It&#39;s always the case that you have to program/design to the spec and test your contraptions as thorouly as possible, but that&#39;s neither adequate excuse for designing an api to make shooting off your own foot more likely by introducing unexpected behavior in corner-cases, nor for elliding an api which does &#39;The Right Thing&#39;TM, by being non-blocking with asynchronous notifications.</p>
<p>Actually, there are These Ways for IO:</p>
<p>1 Blocking Synchronous (Old-Fashioned, Easiest)</p>
<p>2 Non-Blocking Synchronous (Poll or Select) (Poll would burn cycles needlessly)</p>
<p>3 Non-Blocking Asynchronous (Done immediately and/or asynchronous notification)</p>
<p>Windows Currently implements 1 and 2, NOT 3, but (roulette 1 or 3) which looks and sometimes feels like 3, until it bites you in the ass hard, meaning you have to go multithreaded.</p>
<p>Does someone have anything to add to the overview of IO-methodologies, or a correction?</p>
<div class="post">[<i>Not sure what you&#39;re asking me to say. Do you want me to say &quot;The file system guys are lazy bums&quot;? -Raymond</i>]</div>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-937733">
				<div id="div-comment-937733" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Deduplicator</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937733">
			September 28, 2011 at 12:00 pm</a>		</div>

		<p>@Raymond: Well, it&#39;s a start to accept that the current state of affairs is a bit suboptimal, as it forces everyone wanting a responsive UI/other Channel into taking out the big multithreading-sledgehammer.</p>
<p>And I actually don&#39;t think they are lazy bums, even if their decision for what they implemented might only make sense from the provider-side, eg. making their lives easier, without making all that much sense from the user-side or even the resource-side, as it doesn&#39;t allow reaping all the benefits, without using kernel-managed user-mode multi-threading as a massive clunky workaround.</p>
<p>BTW: Did you find a fourth IO-Method? (Not Memmap, please. That would be 1)</p>
<div class="post">[<i>I tend to write about <span style="text-decoration:underline;">practical</span> programming, which means &quot;The goal is to solve a problem. Arguing over whether something was a good idea doesn&#39;t help you solve your problem (at least not until time travel is perfected).&quot; -Raymond</i>]</div>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-937823">
				<div id="div-comment-937823" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Simon Farnsworth</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937823">
			September 29, 2011 at 1:11 am</a>		</div>

		<p>@Deduplicator:</p>
<p>I&#39;m sorry; I thought this was &quot;The Old New Thing&quot;, Raymond Chen&#39;s blog on how things actually are, and why they&#39;re imperfect. I didn&#39;t realise I&#39;d stumbled into the &quot;redesign Windows to remove its warts&quot; blog.</p>
<p>I therefore assumed that you&#39;d understood that this sort of post is about why something that looks odd now got into that state, and were interested in understanding what sort of historical reasoning would lead Microsoft&#39;s engineers to designing an async I/O interface with the specific warts it has, as against some form of &quot;perfect&quot; async I/O interface.</p>
<p>Having implemented such an interface on an embedded system, I sympathise with the warts the Windows async I/O interface has &#8211; they&#39;ve done something simple to implement, that&#39;s usable by the intended target audience (server writers), and has one predictable gotcha, as against a myriad of slightly different weird side-effects of things not going to plan</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-937833">
				<div id="div-comment-937833" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Deduplicator</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937833">
			September 29, 2011 at 2:37 am</a>		</div>

		<p>Yes, this is certainly the old new thing.</p>
<p>But it isn&#39;t only about why things came to be as they are and how they really work, but also how to hack around the limitations without quite violating the spec, and sometimes a bit about different alternatives (implemented/in progress/perhaps in the future/discarded) or completely unrelated topics.</p>
<p>That said, this weird side-effect could probably be removed without breaking anything, and I hope it will (some day, some way).</p>
<p>Even though time-travel would be nice, it&#39;s not really needed here&#8230;</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-937863">
				<div id="div-comment-937863" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Ben</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937863">
			September 29, 2011 at 7:45 am</a>		</div>

		<p>@Simon Farnsworth: &quot;has one predictable gotcha&quot;</p>
<p>As far as I understand this all, Windows does not follow a predictable, well-documented concept for supporting asynch I/O by &quot;overlapped&quot;. I don&#39;t get why you seem to be happy with behavior that you cannot rely on. How do you write your server software, when the asynch behavior can be different on any machine, depending on the disk driver, filesystem provider, filter drivers or the versions of some system DLL you have never heard of?</p>
<p>Someone may want to use your software on exFAT or some other non-NTFS filesystem. If this does not support asynch at all (&quot;converting&quot; everything to sync I/O), your software can hopefully still run without errors, but maybe unusable slow.</p>
<p>Sure, complaining here about this does not change anything, but I take this also as a opportunity to discuss concepts of programming or APIs.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-937993">
				<div id="div-comment-937993" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Simon Farnsworth</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-937993">
			September 29, 2011 at 12:08 pm</a>		</div>

		<p>@Ben:</p>
<p>Writing the server software is not hard, precisely because there is only one gotcha; make sure it is correct if all the async calls turn out synchronous, and make sure it is also correct if they all turn out async. If they all turn out synchronous, you&#39;ve not gained the performance boost async I/O is supposed to give you. If they all turn out async, you have.</p>
<p>If you need asynchronous behaviour for correctness, you use threading primitives to get you asynchronous behaviour. The OVERLAPPED async I/O API lets you exploit hardware parallelism cheaply, if (and only if) that parallelism is easy for the OS to expose.</p>
<p>Remember that big I/O systems are capable of huge I/O parallelism; I&#39;ve not dealt with a really big one, but a small EMC SAN will happily permit you to have 60,000 I/O operations outstanding at once, while a cheap SATA disk will only allow you 30-odd operations at once (and IDE disks only let you have one operation outstanding at a time). The point of the async I/O API is to let you write code that scales trivially from 1 operation at a time to 60,000 operations at a time, depending on the hardware the user puts it on.</p>
<p>I&#39;m not sure why &quot;unusable slow&quot; applies specifically to this API, either &#8211; a user could choose to run your software on a machine without enough RAM for you, so that it&#39;s paging all the time, or in a VM with a tiny timeslice allocated to it, so it has next to no CPU time available. You use async I/O so that if the customer puts your software on a beefy setup, they get high performance, without you having to cope with scaling between 2 threads (one for I/O, one for compute) and 60,000 threads (60 for compute, 59,940 for I/O).</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-938043">
				<div id="div-comment-938043" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Deduplicator</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-938043">
			September 29, 2011 at 12:53 pm</a>		</div>

		<p>@Simon:</p>
<p>Well, if any client allways runs into blocking behavior, your thread-number will go through the roof, meaning you measure your performance by thread-switches/Second, the actual I/O-performance the hardware allows is completely beside the point. Real asynchronous apis provided by kernel-land would avoid that easily.</p>
<p>Much worse, if only part of the requests potentially performed by a client during his session grab and strangle a thread, that would trivially lead to a really-hard-to-find denial-of-service for the service, if not the whole machine. Not only that nobody could connect anymore, but all clients not having entailed their own private thread yet will be starved to death. And those who aren&#39;t starved will move slower than any snails I&#39;ve ever seen. This sudden freezing due to explosion of resource demand could also be trivially avoided using kernel-provided asynchronous apis.</p>
<p>Next, using the software on your pocket-calculator is a strawman if I ever heard one. And where does it say that asynchronous requests are always directly moved to dedicated hardware? Software-managed queues are always used to establish or extend queueing of requests, especially IO.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-938223">
				<div id="div-comment-938223" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Simon Farnsworth</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-938223">
			September 30, 2011 at 4:30 am</a>		</div>

		<p>@Deduplicator:</p>
<p>Why exactly would my thread number go through the roof? Imagine a trivial fully async implementation based around two threads and two queues; thread 1 puts requests into one queue, and picks up completions from another queue at a time to suit it. Thread two picks up requests from one queue, executes them, and puts completions into the second queue. Voil, you have non-blocking asynchronous I/O, with only one request outstanding in hardware at a time. If you want more requests in hardware, replace thread 2 with a constrained size pool of worker threads.</p>
<p>The OVERLAPPED API is still useful; it lets you give each thread in your pool a multiplier effect on beefy enough systems &#8211; you are still guaranteed that each thread can queue one request into the OS, but if some requests complete asynchronously, you get more than one request queued by each thread. In turn, this means that a tiny thread pool can, under the right conditions, queue millions of requests into hardware that&#39;s capable of handling it; compare that to trying to run millions of threads.</p>
<p>Moving the complexity into the kernel doesn&#39;t change anything &#8211; a kernel async API would provide you with a hidden thread pool behaving in the same way as the implementation I&#39;ve described. And you can implement it as a library you take with you from project to project &#8211; if it was a common requirement, it might even become a library supplied with the Platform SDK.</p>
<p>And the &quot;pocket calculator&quot; strawman is from the &quot;OVERLAPPED is useless&quot; side of the argument &#8211; the claim made was that the async I/O API was useless because I could find hardware on which it was always synchronous, and thus would be unusably slow because the application would only be fast enough if the I/O was asynchronous. However, I can make the same argument about any other constrained resource, and I have examples of applications which successfully use this API to gain performance (Microsoft SQL Server), so clearly the API is usable.</p>
<p>I&#39;m also unable to understand your &quot;denial of service&quot; rant &#8211; if the async I/O API opens up a denial of service to all users of the API, please explain how I&#39;d use it on SQL Server. If not, it seems to be &quot;if people write buggy code that assumes unlimited resources, things go wrong&quot;. That would still apply even if all async I/O operations were guaranteed to be async.</p>
<p>You&#39;re also ignoring one more part of the async I/O API &#8211; it&#39;s permissible for it to block for minutes, then return an asynchronous completion, depending on the resources available on the machine. If that&#39;s unacceptable, you need a second thread of execution, and it makes it simpler for &nbsp;if you have to ask for that thread explicitly, rather than having the kernel do some magic multithreading dance underneath you so that you have two threads, but you think you only have one.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-938243">
				<div id="div-comment-938243" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Joe</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20110923-00/?p=9563#comment-938243">
			September 30, 2011 at 6:35 am</a>		</div>

		<p>&quot;Moving the complexity into the kernel doesn&#39;t change anything &#8211; a kernel async API would provide you with a hidden thread pool.&quot;</p>
<p>First, it&#39;s already within the kernel. In the end, it has to serialize even heavy access to functionality and/or resources between different threads/processes, and therefore needs to use queues (explicit, or implicit via Wait functions) to collect and serve all the outstanding concurrent I/O requests to one resource. (Site note: There are different blocking needs which you cannot all reduce to &quot;hardware parallism&quot;: compare &quot;write disk block&quot; to &quot;rename file&quot; to &quot;accept TCP connection&quot;).</p>
<p>How it serves this queued-up requests, via worker threads or via interrupt handling or events or mutexes or what else, is really not important for the high-level APIs, like ReadFile/WriteFile/SetEndOfFile and so on.</p>
<p>Second, it would be much easier to use &quot;overlapped&quot;, if there would be no weird corner cases, and if every issued overlapped operation would be completed only asynchronous (the completition ports are very handy). It should save you from coding for both cases, the sync and the async completition. It would be much easier if each and every overlapped call be non-blocking: The request is queued up to the kernel, your thread continue, and the result of the operation is signalled later on.</p>
<p>In the end, why code this complexity go into every application (which will get this wrong most of the time)? The overlapped concept should be implemented in the kernel in a coherent way, without special cases and without a strange mix of sync and async completition.</p>

		
				</div>
		</li><!-- #comment-## -->
		</ol><!-- .comment-list -->

		<div class="navigation pagination">
					</div>

	
			<p class="no-comments">Comments are closed.</p>
<!-- COMMENTS END -->
</div></td></tr></table>