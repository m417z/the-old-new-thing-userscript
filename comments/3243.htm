<table class="commenttable" cellspacing="0" cellpadding="0"><tr><td><div class="commentdiv"><div class="commentdivhdr">
<!-- COMMENTS START -->
Comments (23)	</div>

	
			<div class="navigation pagination clear-both">
					</div>

		<ol class="comment-list">
					<li class="comment even thread-even depth-1" id="comment-1077623">
				<div id="div-comment-1077623" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Axel</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1077623">
			September 13, 2013 at 7:24 am</a>		</div>

		<p>ARM is always hyped as being so much better than x86 but the relaxed memory semantics of it just make everything so much more difficult to code for in the age of multicores.</p>
<p>Intel made the right choice of being very strict about that even if it means you lose a bit of performance.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-1077633">
				<div id="div-comment-1077633" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Joshua</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1077633">
			September 13, 2013 at 7:31 am</a>		</div>

		<p>What&#39;s really annoying is the old Dekker&#39;s algorithm and friends don&#39;t work on ARM.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-1077663">
				<div id="div-comment-1077663" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">laonianren</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1077663">
			September 13, 2013 at 8:44 am</a>		</div>

		<p>@Joshua. &nbsp;Dekker&#39;s algorithm doesn&#39;t work on Intel either, but it fails less frequently than on ARM so you might not notice:</p>
<p><a rel="nofollow" target="_new" href="http://jakob.engbloms.se/archives/65">jakob.engbloms.se/&#8230;/65</a></p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-1077713">
				<div id="div-comment-1077713" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">j b</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1077713">
			September 13, 2013 at 11:33 am</a>		</div>

		<p>My favorite hardware solution in terms of low conceptual complexity (and I would think implementation was fairly simple, too): The Norsk Data ND-5000 superminis (&quot;VAX class&quot; machines) had a SOLO instruction that turned off all interrupts and started an 8-bit counter incremented every clock cycle. A TUTTI instruction reenabled interrupts and stopped the counter. &nbsp;A counter overflow caused a fatal interrupt to the current process, so it had at most 255 clock cycles to complete its protected operations. The overhead was one clock cycle each for the SOLO and TUTTI instructions; all instructions executed at full speed. If the protected operations were to complex to complete in 255 cycles, you would have to build some sort of critical region or monitor, protected by a semaphore read/update in SOLO-TUTTI sequences, and the overhead would double to 4 clock cycles, 2 for entering the region, 2 for leaving. Still quite moderate.</p>
<p>Interrupts signaled between SOLO and TUTTI were queued up. In theory the worst case extra latency was 256 cycles, but in pracice, duration of SOLO-TUTTI sequences were more like a dozen clock cycles or less (and usually, external interrupts were processed by a 16-bit front end with super-low interrupt latency, down to 900 ns for a complete context switch). </p>
<p>The best thing was that even though SOLO disabled interrupts (for a few clock cycles), these instructions were unprivileged and could be useed by any process in user space, without incurring any context switch &#8211; not even a function call, if you didn&#39;t want to. SOLO and TUTTI were available as keywords in the systems programming language to any application programmer (other languages usually had a macro facility and could use inline assembly, looking very much the same), making the cost of protecting updates of shared data structures very low. </p>
<p>Even the Intel LOCK prefix is unprivileged, but you need help from your compiler to generate it, you can use it only with specific instructions and you can&#39;t extend the lock to an instruction sequence e.g. relinking a linked list, which requires several instructions. And it complicates instruction decoding by making the instruction format even more complex in structure.</p>
<p>In principle, SOLO-TUTTI could be abused by &quot;terrorists&quot; creting a loop of a SOLO instruction, do dummy work for 230-240 cycles, before a TUTTI. Then then could keep interrupts disabled for a significant fraction of their time slice. However, the SOLO would allow handling of all interrupts arriving during this loop iteration, before the next loop iteration, so the effect would probably be far less than expected by the students trying to show their cleverness. (Who else would care to do antyning of that sort?)</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment byuser comment-author-alegrigoriev even thread-even depth-1" id="comment-1077723">
				<div id="div-comment-1077723" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/alegr1' rel='external nofollow' class='url'>alegr1</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1077723">
			September 13, 2013 at 11:34 am</a>		</div>

		<p>The IA32 implements interlocked operations for multiple nodes/CPU, using the cache coherency protocol. The data being modified is in the local cache only. The cache coherency protocol makes sure no other processor contains a copy of the stale cache line.</p>
<p>This makes interlocked operations no more expensive than a simple non-interlocked load-store-modify operation. There is a bit of cost, though, because interlocked operations are serializing.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-1077743">
				<div id="div-comment-1077743" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Myria</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1077743">
			September 13, 2013 at 12:42 pm</a>		</div>

		<p>InterlockedIncrement on x86-32 is lock xadd, then inc to give the caller the result it expects.</p>
<p>Somewhat relevant: On x88-32, InterlockedIncrement64 is implemented using a loop that reads the variable into two registers, increments the 64-bit value, then attempts to do a lock cmpxchg8b (i.e., InterlockedCompareExchange64) to write the new value if the old value has not changed since it was read. &nbsp;If the cmpxchg8b fails, it retries the loop. &nbsp;This works because livelock is essentially impossible (probability tends toward zero) in modern environments. &nbsp;The ABA problem is there, but is irrelevant because you&#39;re merely incrementing an integer. &nbsp;You generally aren&#39;t going to mess up if another thread manages to wrap the counter between you reading the value and writing the incremented version (and with a 64-bit counter, good luck with that anyway).</p>
<p>Intel has new processors coming (or out?) with transactional memory features that look a lot like load-linked/store-conditional but for multiple memory locations. &nbsp;It&#39;s hard to say what&#39;s going to happen with that. &nbsp;It&#39;s certainly not going to be widely used any time soon.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-1077753">
				<div id="div-comment-1077753" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">Myria</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1077753">
			September 13, 2013 at 1:04 pm</a>		</div>

		<p>@Axel: Yes, strict memory ordering is nice, but it is really only an x86 thing these days. &nbsp;I&#39;d much rather have my code able to run on many different systems.</p>
<p>@j b: &quot;TUTTI&quot;? &nbsp;That&#39;s just as bad as PowerPC&#39;s &quot;EIEIO&quot; instruction =^_^=</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-1077783">
				<div id="div-comment-1077783" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">SimonRev</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1077783">
			September 13, 2013 at 1:48 pm</a>		</div>

		<p>Wow, this article is particularly relevant to me today, as we are reviewing customer code that is misusing InterlockedIncrement on an ARM Windows CE platform.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment byuser comment-author-alegrigoriev even thread-even depth-1" id="comment-1077793">
				<div id="div-comment-1077793" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/alegr1' rel='external nofollow' class='url'>alegr1</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1077793">
			September 13, 2013 at 1:53 pm</a>		</div>

		<p>@Myria:</p>
<p>I don&#39;t see ABA problem in that implementation of II64. Where it is?</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-1077803">
				<div id="div-comment-1077803" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">j b</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1077803">
			September 14, 2013 at 9:40 am</a>		</div>

		<p>@Myria, </p>
<p>You never played in an orchestra, I presume.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment byuser comment-author-alegrigoriev even thread-even depth-1" id="comment-1077813">
				<div id="div-comment-1077813" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/alegr1' rel='external nofollow' class='url'>alegr1</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1077813">
			September 14, 2013 at 11:42 am</a>		</div>

		<p>&gt;Then then could keep interrupts disabled for a significant fraction of their time slice.</p>
<p>I suppose TUTTI will handle all pending interrupt requests, so the only effect will be 240 clocks of interrupt latency (and timeslice occasionally extended by up to 240 clocks).</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment byuser comment-author-wndsks odd alt thread-odd thread-alt depth-1" id="comment-1077823">
				<div id="div-comment-1077823" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/skSdnW' rel='external nofollow' class='url'>skSdnW</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1077823">
			September 15, 2013 at 7:47 am</a>		</div>

		<p>@Myria On Win95 it is LOCK INC and not XADD, this is also why the COM rules for the AddRef/Release return value are the way they are&#8230;</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-1077833">
				<div id="div-comment-1077833" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">j b</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1077833">
			September 15, 2013 at 9:04 am</a>		</div>

		<p>@alegr,</p>
<p>I meant to write exactly what you point out, but wrote SOLO instead of TUTTI, which messed the thing up. Sorry. Your are right; the only &quot;damage&quot; would be as you say. But when this machine came out, me and my fellow students were hoping for much more&#8230;</p>
<p>One point that came to mind: Each CPU had a private cache and usually private RAM. &quot;Multiport&quot; RAM could also be shared between several CPUs, managed by dedicated arbiting hardware. I take for granted that during a SOLO-TUTTI sequence, an access to the shared memory would lock it for access through other ports until TUTTI; that could actually slow down other CPUs competing for access to the same shared memory. I never heard this being raised as an issue, so programmers probably avoided it by not accessing the shared RAM in SOLO-TUTTI sequnces. (I think all customers running mulitport RAM modules were highly specialized single-application environments with expert programners who knew what they were doing :-).)</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-1077843">
				<div id="div-comment-1077843" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='http://blogs.msdn.com/arcangelpip_4000_hotmail.com/ProfileUrlRedirect.ashx' rel='external nofollow' class='url'>arcangelpip@hotmail.com</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1077843">
			September 15, 2013 at 3:24 pm</a>		</div>

		<p>@skSdnW:</p>
<p>Because as we both know, XADD was added with the 486, COM predates that, and Windows 95 could run on a 386.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-1077873">
				<div id="div-comment-1077873" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">voo</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1077873">
			September 16, 2013 at 4:37 am</a>		</div>

		<p>@Axel If you care about the underlying platform memory model you&#39;re most likely doing it wrong anyhow. Most modern languages have their own memory model by now that dictates what you can and can&#39;t expect. By the same definition no, Dekker&#39;s algorithm is not broken the author of the linked article just doesn&#39;t understand what volatile does and does not do in C/C++. If your language tells you that volatile reads/writes also have a uni-directional memory barrier the algorithm will work just fine. C++ doesn&#39;t, Java5+ and .NET do.</p>
<p>Really the only people that have to care about memory models these days are compiler writer and now let me go back to reading the Aarch64 specification, because some people actually have to write those backends for the rest ;)</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-1077953">
				<div id="div-comment-1077953" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">DWalker</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1077953">
			September 16, 2013 at 11:14 am</a>		</div>

		<p>I always thought that the silicon itself could be engineered to be able to increment a value at a storage location. &nbsp;But I suppose that involves actually changing some bits in memory (RAM) with the appropriate semantics (carries, etc). &nbsp;Is it true that the only things that RAM chips can do is allow read and write?</p>
<p>Still, if this could be implemented directly in RAM hardware, wouldn&#39;t that be useful?</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment byuser comment-author-bboorman even thread-even depth-1" id="comment-1077973">
				<div id="div-comment-1077973" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/Brian_EE' rel='external nofollow' class='url'>Brian_EE</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1077973">
			September 16, 2013 at 12:02 pm</a>		</div>

		<p>@DWalker: If you mean the actual RAM attached to the processor (whether internal RAM, external RAM) and not processor registers, then No, the RAM cannot do anything but store and retrieve the bit value.</p>
<p>Now, theoretically, could a memory chip be designed to increment and decrement a value in place? Sort of. The chip could internally do a read/modify/write without exposing the value on the bus to run it through some add/subtract block. But there are a couple of issues:</p>
<p>&nbsp;1) how do you instruct the memory to do this? There would need to be some standard that specified how to make this work. That might be ok for DRAM-based memory systems such as DDRx but is practically impossible if your memory is SRAM (as some embedded systems use)</p>
<p>&nbsp;2) how does the memory know the correct width of the element you want to increment?</p>
<p>I am sure there are more issues that I am too lazy to think of at the moment.</p>
<div class="post">[<em>The biggest issue is that the increment doesn&#39;t happen in RAM. It happens on the CPU itself inside the L1 cache. So make your RAM as fancy as you like. It doesn&#39;t matter. The value will be incremented inside the CPU&#39;s L1, and then it will get flushed out some unspecified time later. I guess you could design a CPU that, when it sees an increment instruction, flushes out the cache line, instructs other CPUs to flush out the cache line, then issues the &quot;increment directly in RAM&quot; instruction. But I think it&#39;ll be much slower than what they do now. -Raymond</em>]</div>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-1077983">
				<div id="div-comment-1077983" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">DWalker</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1077983">
			September 16, 2013 at 12:22 pm</a>		</div>

		<p>@Brian: Right, I realize that the RAM would have to know the expected width of the element, and there would need to be a new RAM or memory bus &quot;instruction&quot; defined in addition to &quot;read&quot; and &quot;write&quot;, and so on. &nbsp;The increment (or decrement) instruction would have to specify the width.</p>
<p>I was just speculating that it might be an interesting thing to do.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-1078013">
				<div id="div-comment-1078013" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">DWalker</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1078013">
			September 16, 2013 at 1:06 pm</a>		</div>

		<p>@Raymond. &nbsp;Yes, there&#39;s that too. &nbsp;:-) &nbsp;Oh, the good old days of uniprocessors with no shared memory.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-1078023">
				<div id="div-comment-1078023" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">DWalker</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1078023">
			September 16, 2013 at 3:06 pm</a>		</div>

		<p>Of course, the compare-exchange is also comparing what&#39;s in the CPU&#39;s L1 cache with &#8230; what&#39;s in every other CPU&#39;s L1 cache? &nbsp;Isn&#39;t a flush or a memory barrier required anyway, for a compare-and-swap (or for any compare with memory)?</p>
<div class="post">[<em>A cache invalidation occurs on all CPUs other than the one performing the operation. How this is accomplished varies from processor to processor. -Raymond</em>]</div>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment byuser comment-author-alegrigoriev even thread-even depth-1" id="comment-1078033">
				<div id="div-comment-1078033" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/alegr1' rel='external nofollow' class='url'>alegr1</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1078033">
			September 16, 2013 at 4:34 pm</a>		</div>

		<p>@Dwalker:</p>
<p>&gt;Of course, the compare-exchange is also comparing what&#39;s in the CPU&#39;s L1 cache with &#8230; what&#39;s in every other CPU&#39;s L1 cache?</p>
<p>As I mentioned above, interlocked operations in IA32 processors are based on the cache coherency protocol. The cache coherency protocol guarantees that for a dirty cache line, there is no other copy in other nodes. The processor only needs to read and modify its local cache line atomically.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment byuser comment-author-alegrigoriev odd alt thread-odd thread-alt depth-1" id="comment-1078283">
				<div id="div-comment-1078283" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href='https://social.msdn.microsoft.com/profile/alegr1' rel='external nofollow' class='url'>alegr1</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1078283">
			September 18, 2013 at 10:26 am</a>		</div>

		<p>&gt;but &#8230; couldn&#39;t the CPU doing an &quot;Atomic increment&quot; use the cache coherency protocol to make sure that there is no other copy in other nodes, and then increment the value in its own cache?</p>
<p>This is exactly what happens. If the cache line is marked as &quot;shared&quot;, any modification on it will cause invalidation.</p>
<p>Flushing it to RAM every time would be slow. Because RAM is slow.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-1078263">
				<div id="div-comment-1078263" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn">DWalker</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/oldnewthing/20130913-00/?p=3243#comment-1078263">
			September 18, 2013 at 7:54 am</a>		</div>

		<p>@Alegr1: I am not a chip engineer, but &#8230; couldn&#39;t the CPU doing an &quot;Atomic increment&quot; use the cache coherency protocol to make sure that there is no other copy in other nodes, and then increment the value in its own cache? &nbsp;Then, when that cache line gets flushed to RAM, the value will be incremented. &nbsp;</p>
<p>Raymond said that would likely be slower than what&#39;s done now, but I don&#39;t see how. &nbsp;Does the Compare-exchange function actually compare a register with what&#39;s in its own cache line, rather than what&#39;s actually in RAM?</p>
<div class="post">[<em>No, what I said would probably be slower is for *all* CPUs to invalidate their cache lines (even the one doing the incrementing) so that the RAM could do a hardware increment. -Raymond</em>]</div>

		
				</div>
		</li><!-- #comment-## -->
		</ol><!-- .comment-list -->

		<div class="navigation pagination">
					</div>

	
			<p class="no-comments">Comments are closed.</p>
<!-- COMMENTS END -->
</div></td></tr></table>